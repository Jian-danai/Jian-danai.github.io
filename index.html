<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Bianjiang Yang</title>
  
  <meta name="author" content="Bianjiang Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/favicon_ntu.ico">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Bianjiang Yang &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  <!--<img style="vertical-align:middle" src='image/wangtan_name.gif' height='50px' width='WIDTHpx'>-->
		  </name>
        </p>
		<p>&nbsp &nbsp Bianjiang Yang has graduated from the <a href="http://www.cse.zju.edu.cn/english/">College of Control Science and Engineering</a> 
			& <a href="http://ckc.zju.edu.cn/ckcen/_t1906/main.htm">Chu Kochen Honors College</a> at
			<a href="https://www.zju.edu.cn/english/">Zhejiang University (ZJU)</a> with an honored bachelor 
			degree major in Automation and Mixed Honors Class. He used to be an exchange student at
			<a href="http://scse.ntu.edu.sg/Pages/Home.aspx">School of Computer Science and Engineering, 
				Nanyang Technological University (NTU)</a> from Aug 2018 to Dec 2018. <br/>
			&nbsp &nbsp His primary research interests are unsupervised Learning topics especially those related to 3D Computer Vision, Rendering,
Generative Models, and Model Compression. He is also interested in Cognitive Science and applications of Machine Learning in Social Science.<br/>
			&nbsp &nbsp He will pursue his doctoral studies and do valuable research in related fields at <a href="https://www.purdue.edu/">Purdue University</a> 
			with Prof. <a href="https://web.ics.purdue.edu/~qqiu/">Qiang Qiu</a>.
		</p>
		
        </p>


        <p align=center>
          <a href="mailto:jiandanaismail@gmail.com">Email</a> &nbsp/&nbsp
          <!--<a href="image/cv_wangtan.pdf">CV</a> &nbsp/&nbsp-->
          <a href="https://www.linkedin.com/in/bianjiang-yang-776377179/">LinkedIn</a> &nbsp/&nbsp
	  <a href="https://www.researchgate.net/profile/Bianjiang_Yang">ResearchGate</a> &nbsp/&nbsp
	  <a href="https://scholar.google.com/citations?user=8zbtVhUAAAAJ&hl=zh-CN">Google Scholar</a> 

        </p>

        </td>
        <td width="33%">
        <img src="image/yangbj.jpg" width="100">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
		  <li> <strongsmall>[2020/08]</strongsmall> &nbsp;&nbsp;<smalll>Admitted into the PhD program at ECE, Purdue University.</smalll><br/>
		  <li> <strongsmall>[2020/08]</strongsmall> &nbsp;&nbsp;<smalll>1 paper(Oral Presentation) with Prof. Haoji Hu and Miss Zi Hui is accepted by VCIP 2020 which will be held in Macau.</smalll><br/>
<li> <strongsmall>[2019/08]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Yang Zhao and Mr. Baizhe He is accepted by the Journal of Chemical Physics.</smalll><br/>          <!--
	  <li> <strongsmall>[2019/03/23]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Prof. Alan Hanjalic submitted to ICCV19.</smalll><br/>
          <li> <strongsmall>[2019/02/25]</strongsmall>&nbsp;&nbsp; <smalll>CVPR boardline reject. Better than AAAI. Revise it and try again! </smalll><br/>
          <li> <strongsmall>[2018/12/24]</strongsmall> &nbsp;&nbsp;<smalll>1 revised paper (from AAAI19) submitted to TNNLS. </smalll><br/>
          <li> <strongsmall>[2018/11/13]</strongsmall> &nbsp;&nbsp;<smalll>1 new paper submitted to CVPR19. </smalll><br/>
          <li> <strongsmall>[2018/11/01]</strongsmall> &nbsp;&nbsp;<smalll>My fisrt paper is regected by AAAI19 :(  Keep going! </smalll><br/>
          <li> <strongsmall>[2018/09/08]</strongsmall> &nbsp;&nbsp;<smalll>My fisrt paper submitted to AAAI19.</smalll><br/> 
          -->
          
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education & Internship</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/zju_iron.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Zhejiang University (ZJU), China</stronghuge><br />
          Bachelor's Degree (Honours) in Automation &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - July. 2020 <br />
          Mixed Class of Chu Kochen Honors College.<br />
          Supervisor: Prof. <a href="https://person.zju.edu.cn/en/lujg">Jiangang Lu</a>.  &nbsp;&nbsp; Collaborated with Prof. <a href="https://person.zju.edu.cn/en/huhaoji">Haoji Hu</a>.
          </p>
        </td>
      </tr>
	  
	  
	    <tr>
          <td width="10%">
            <img src='image/ntu_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Nanyang Technological University (NTU), Singapore</stronghuge><br />
          Exchange Program, School of Computer Science and Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2018 - Dec. 2018 <br />
          Supervisor: Prof. <a href="https://www3.ntu.edu.sg/home/zhaoyang/index.html">Yang Zhao</a>
          </p>
        </td>
      </tr>
	      
	      
	    <tr>
          <td width="10%">
            <img src='image/rose.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Nanyang Technological University (NTU), Singapore</stronghuge><br />
          Internship, ROSE Lab &nbsp;&nbsp;&nbsp;&nbsp; &bull; Jul. 2019 - Oct. 2019 <br />
          Supervisor: Dr. <a href="https://q-zh.github.io/">Qian Zheng</a>
          </p>
        </td>
      </tr>	  
	      
	        <tr>
          <td width="10%">
            <img src='image/PurdueLogo.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Purdue University (Purdue), USA</stronghuge><br />
          PhD candidate(Ross Fellowship), School of Electrical and Computer Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2021 -  <br />
	  Research area: Machine Learning and Computer Vision <br />
          Supervisor: Prof. <a href="https://web.ics.purdue.edu/~qqiu/">Qiang Qiu</a>
          </p>
        </td>
      </tr>
	      
	      
      </table>

<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication & Manuscript</heading>
        </td>
      </tr>
      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/compress_beauty.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">

	 <strong>Compressing Facial Makeup Transfer Networks by Collaborative Distillation and Kernel Decomposition</strong><br>
       <strong>Bianjiang Yang</strong>,
    Zi Hui,
    <a href="https://person.zju.edu.cn/en/huhaoji">Haoji Hu</a>,
	      Xinyu Hu,
      Lu Yu <br>
        <em>IEEE International Conference on Visual Communications and Image Processing</em>, <strong>VCIP 2020(Oral)</strong>, <a href="https://ieeexplore.ieee.org/document/9301756"><strong>[Paperlink]</strong></a> <a href="https://github.com/Jian-danai/Decompose-Distill-BeautyGAN"><strong>[Code]</strong></a><br>
        <em>Area: Facial Makeup Transfer, Network Compression, Knowledge Distillation, Convolutional Kernel Decomposition</em> <br>
        <p></p>
		<p>The main idea of collaborative distillation is underpinned
by a finding that the encoder-decoder pairs construct an exclusive
collaborative relationship, which is regarded as a new kind of
knowledge for low-level vision tasks. For kernel decomposition,
we apply the depth-wise separation of convolutional kernels to
build a light-weighted Convolutional Neural Network
from the original network. </p>
      </td>
    </tr>
   </table>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/lz-nn.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">

	 <strong>Applications of Neural Networks to Dynamics Simulation of Dissipative Landau-Zener Transitions</strong><br>
       <strong>Bianjiang Yang</strong>,
    Baizhe He,
	      <a href="https://www.linkedin.com/in/jiajunwan"> Jiajun Wan</a>,
	       <a href="https://www.linkedin.com/in/kubalsharvaj/">Sharvaj Kubal</a>,
    <a href="https://www3.ntu.edu.sg/home/zhaoyang/index.html">Yang Zhao</a> <br>
        <em>The Journal of Chemical Physics </em>, <a href="https://www.sciencedirect.com/science/article/pii/S030101041930802X"><strong>[Paperlink]</strong></a> <a href="https://github.com/Jian-danai/LZ-NN-code"><strong>[Code]</strong></a><br>
        <em>Area: Dissipative Landau-Zener model, Recurrent Neural network, Quantum dynamics</em> <br>
        <p></p>
		<p>We simulate the dissipative dynamics of the Landau-Zener (LZ) model in 2D and 3D by employing the nonlinear autoregressive neural
network and the long short-term memory neural network. </p>
      </td>
    </tr>
   </table>
	  
	  
	  <p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Awards</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Ross Fellowship</stronghuge> &nbsp; 2021<br/>
		  <li> <stronghuge>Scholarship for Outstanding Academic Performance</stronghuge> &nbsp; 2018<br/>
          <li> <stronghuge>Scholarship for Outstanding Students</stronghuge> &nbsp; 2019<br/>
          <li> <stronghuge>Outstanding Student Honor</stronghuge> &nbsp; 2019<br/>
          <li> <stronghuge>First-class Scholarship for Research and Innovation</stronghuge> &nbsp; 2019<br/>
          <li>  <stronghuge>Social Work Scholarship</stronghuge> &nbsp; 2019 <br/>
          <li> <stronghuge>Supcon Scholarship</stronghuge> &nbsp; 2019<br/>
          <li> <stronghuge>Social Practice Scholarship</stronghuge> &nbsp; 2017<br/>
		  <li> <stronghuge>Shu Ping Scholarship</stronghuge> &nbsp; 2016~2019<br/>
		  <li> <stronghuge>Third Prize: The 16th ”Challenge Cup” College Students’ Extracurricular Academic Science and Technology
Contest of China</stronghuge> &nbsp; 2019<br/>
			  <li> <stronghuge>First Prize: The 16th ”Challenge Cup” College Students’ Extracurricular Academic Science and Technology
Contest of Zhejiang Province</stronghuge> &nbsp; 2019<br/>
		  <li> <stronghuge>Bronze Award: The 5th China ”Internet+” College Students Innovation and Entrepreneurship Competition of
Zhejiang Province</stronghuge> &nbsp; 2019<br/>
		  <li> <stronghuge>Top Prize: ”Bank of Communications Cup” Graduate Smart City Technology and Creative Design Competition
of Zhejiang University</stronghuge> &nbsp; 2019<br/>
          </p>
          </div>
        </td>
      </tr>
</table>
	
	  <p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Patents & Software Copyrights</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>(CN 201910382747.0) An Online Drowning Behavior Identification Method Based on Machine Vision</stronghuge> <br/>
		  <li> <stronghuge>(CN 201920655061.X) A multi-angle waterproof camera image acquisition frame</stronghuge> <br/>
          <li> <stronghuge>(CN 2019SR0711843) Industrial diagnostic doctor v1.0</stronghuge> <br/>
          <li> <stronghuge>(CN 2019SR0686911) Life Guard v1.0 (Dual-body dive unmanned system terminal control software for drowning warning and rescue)</stronghuge> <br/>
          <li> <stronghuge>(CN 2019SR0407623) Lower-computer software for drowning warning and rescue v1.0</stronghuge> <br/>
          </p>
          </div>
        </td>
      </tr>
</table>

	  <p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Miscellaneous</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>English Proficiency: IELTS(Academic) 7.0 </stronghuge> <br/>
          <li> <stronghuge>Programming Languages: Python3 / MATLAB / C / Java. </stronghuge> <br/>
          <li> <stronghuge>Other Skills: PyTorch, Blender, Robot Operating System (ROS), LabVIEW. </stronghuge> <br/>
          </p>
          </div>
        </td>
      </tr>
</table>	  
	  
Still in Construction...

<!--
	  
<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="http://cfm.uestc.edu.cn/">
            <img src='image/cfm_icon4.png' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Center For Future Media, UESTC</stronghuge><br />
          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Mar. 2018 - Jun. 2020 <br />
          Advisors: &nbsp; Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>.  &nbsp;&nbsp;Collaborated with  Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a><br/>
          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>
          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>
          <li> Complete 3 works and make the submission.
          </p>
        </td>
      </tr>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://mreallab.github.io/people.html">
            <img src='image/mreal_icon.png' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>MReal Lab, NTU</stronghuge><br />
          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; July. 2019 - Aug. 2020 <br />
          Advisors: &nbsp; Prof. <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang </a>
          </p>
        </td>
      </tr>
  


<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication & Manuscript</heading>
        </td>
      </tr>
      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/framework_github.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">

	 <strong>Visual Commonsense R-CNN</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2020</strong>, <a href="https://arxiv.org/abs/2002.12204"><strong>[Paperlink]</strong></a>, 
		<a href="https://github.com/Wangt-CN/VC-R-CNN"><strong>[Code]</strong></a>, <a href="https://zhuanlan.zhihu.com/p/111306353"><strong>[Zhihu]</strong></a></em><br>
        <em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
        <p></p>
		<p>In this paper, we present a novel un-/self-supervised feature representation learning method, Visual Commonsense Region-based Convolutional Neural Network (VC R-CNN), to serve as an improved visual region encoder for Vision & Language high-level tasks. </p>
      </td>
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='project/vc-rcnn/VC_CVPRW.png'  width="200" height="110">
      </td>
      <td valign="top" width="75%">

	 <strong>Visual Commonsense Representation Learning via Causal Inference (Abstact Version of VC R-CNN)</strong><br>
       <strong>Tan Wang</strong>,
    <a>Jianqiang Huang</a>,
    <a href="https://www.ntu.edu.sg/home/hanwangzhang/">Hanwang Zhang</a>,
      <a href="https://qianrusun.com/">Qianru Sun</a> <br>
        <em>IEEE International Conference on Computer Vision and Pattern Recognition MVM Workshop, <strong>CVPRW 2020</strong>, <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w26/Wang_Visual_Commonsense_Representation_Learning_via_Causal_Inference_CVPRW_2020_paper.html"><strong>[Paperlink]</strong></a>, <a href="https://github.com/Wangt-CN/VC-R-CNN"><strong>[Code]</strong></a>, <a href="https://zhuanlan.zhihu.com/p/111306353"><strong>[Zhihu]</strong></a></em><br>
        <em><strong><font color="#a82e2e">(Oral Presentation)</font></strong></em> <br>
		<em>Area: Visual and Language, Causal Reasoning, Self-supervised Learning</em> <br>
      </td>
    </tr>
   </table>



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/mtfn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">

	 <strong>Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking</strong><br>
       <strong>Tan Wang</strong>,
    <a href="https://interxuxing.github.io/">Xing Xu</a>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
        <em>ACM International Conference on Multimedia, <strong>MM 2019</strong>, Nice, France, October 2019, <a href="https://arxiv.org/abs/1908.04011"><strong>[Paperlink]</strong></a>, <a href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code"><strong>[Code]</strong></a></em><br>
        <em><strong><font color="#a82e2e">(Oral Presentation, 4.96% acceptance rate)</font></strong></em> <br>
        <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable matching performance with acceptable model complexity and much less time consuming. </p>
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/CASC1.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	 
	 
      <strong>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</strong><br>
	  <a href="https://interxuxing.github.io/">Xing Xu*</a>,
      <strong>Tan Wang*</strong>,
	  <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
	  Lin Zuo,
      <a href="http://cfm.uestc.edu.cn/~fshen/">Fumin Shen</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
	      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
          <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>In this paper, we propose a novel hybrid matching approach named Cross-modal Attention with Semantic Consistence (CASC) for image-text matching, which is a joint framework that performs cross-modal attention for local alignment and multi-label prediction for global semantic consistence.</p>
      </td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/radial-gcn.png'  width="200" height="130">
      </td>
      <td valign="top" width="75%">
	  <strong>Cross-Modal Attention with Semantic Consistence for Image-Text Matching</strong><br>     
    <a href="https://interxuxing.github.io/">Xing Xu*</a>,
	<strong>Tan Wang*</strong>,
    <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
      <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a>,
      <a href="http://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a> <br>
      <em>IEEE Transactions on Neural Networks and learning systems, <strong>TNNLS 2020</strong> </em> <br>
      <em>Area: Visual and Language, Image-text matching</em> <br>
        <p></p>
        <p>We propose an innovative answer-centric approach  to focus on the relevant image regions only to reduce the complexity on VQG task.</p>
      </td>
    </tr>
   </table>




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Leadership Experience</heading>
      </td>
      </tr>



      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/lecture.jpg'  width="195" height="130">
      </td>
      <td valign="top" width="75%">
        <stronghuge>Lecture Group of EE Department</stronghuge> </br>
        <huge><em>Founder & President</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Oct. 2017 - Sep. 2018 </br>
        <p></p>
        <p>
         <li> Organized academic forum, sharing sessions, Q&A meetings more than 30 times, serving over 1000 students on studying and future planing.<br/>
          <li> The team grows to 30 people and won the Outstanding Student Organisation prize in 2018.<br/>
          </p> 
      </td>
    </tr>
   </table>


       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="dachuang_stop()" onmouseover="dachuang_start()" >
      <td width="26%">
        <div class="one">
                <div class="two" id='dachuang_image'><img src='image/dachuang2.png'  width="195" height="130"></div>
                <img src='image/dachuang1.png'  width="195" height="130">
              </div>
      <script type="text/javascript">
                function dachuang_start() {
                  document.getElementById('dachuang_image').style.opacity = "1";
                }
                function dachuang_stop() {
                  document.getElementById('dachuang_image').style.opacity = "0";
                }
                dachuang_stop()
              </script>
      </td>

      <td valign="top" width="75%">
        <stronghuge>Innovative Entrepreneurship Project of UESTC</stronghuge> </br>
        <huge><em>Team Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2017 - Mar. 2018 </br>
        <p></p>
        <p>
         <li> This project focus on the pedestrian detection in low-light condition with excellent conclusion. We combine the recent pedestrian detection models with the low-light image enhancement algorithm based on Laplace operator.<br/>
         <li> Responsible for the code implementation and project promotion.<br/>
          </p> 
      </td>
    </tr>
   </table>
          




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)
          </p>
          <p>
          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.
          </p>
      </td>
      </tr>





   <p></p><p></p><p></p><p></p><p></p>
   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>
-->
<!-- end this-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
   
  </body>
</html>
